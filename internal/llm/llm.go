package llm

import (
	"bytes"
	stdcontext "context"
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"strings"

	"github.com/sashabaranov/go-openai"
	"github.com/willis7/prtool/internal/model"
)

// LLM defines the interface for Language Model providers
type LLM interface {
	// Summarise takes a context string containing PR information and returns a summary
	Summarise(context string) (string, error)
}

// StubLLM is a test implementation that returns a fixed summary
type StubLLM struct {
	summary string
	err     error
}

// NewStubLLM creates a new stub LLM with a default summary
func NewStubLLM() *StubLLM {
	return &StubLLM{
		summary: "This is a summary generated by the stub LLM implementation. The development team has been making good progress with several important pull requests merged this period.",
	}
}

// NewStubLLMWithSummary creates a new stub LLM with a custom summary
func NewStubLLMWithSummary(summary string) *StubLLM {
	return &StubLLM{
		summary: summary,
	}
}

// NewStubLLMWithError creates a new stub LLM that returns an error
func NewStubLLMWithError(err error) *StubLLM {
	return &StubLLM{
		err: err,
	}
}

// Summarise implements the LLM interface for the stub
func (s *StubLLM) Summarise(context string) (string, error) {
	if s.err != nil {
		return "", s.err
	}
	return s.summary, nil
}

// BuildContext creates a context string from PR data suitable for LLM processing
func BuildContext(prs []*model.PR) string {
	if len(prs) == 0 {
		return "No pull requests found for the specified criteria."
	}

	var context string
	context += "Pull Request Summary:\n\n"

	for i, pr := range prs {
		context += fmt.Sprintf("%d. %s\n", i+1, pr.Title)
		context += fmt.Sprintf("   Author: %s\n", pr.Author)
		context += fmt.Sprintf("   Repository: %s\n", pr.Repository)

		if pr.MergedAt != nil {
			context += fmt.Sprintf("   Merged: %s\n", pr.MergedAt.Format("2006-01-02"))
		}

		if len(pr.Labels) > 0 {
			context += fmt.Sprintf("   Labels: %s\n", strings.Join(pr.Labels, ", "))
		}

		if pr.Body != "" {
			// Truncate body for context to avoid overly long prompts
			body := pr.Body
			if len(body) > 200 {
				body = body[:200] + "..."
			}
			context += fmt.Sprintf("   Description: %s\n", body)
		}

		context += "\n"
	}

	return context
}

// OpenAILLM implements the LLM interface using OpenAI's API
type OpenAILLM struct {
	client *openai.Client
	model  string
}

// NewOpenAILLM creates a new OpenAI LLM client
func NewOpenAILLM(apiKey, model string) *OpenAILLM {
	if model == "" {
		model = openai.GPT3Dot5Turbo // Default model
	}

	client := openai.NewClient(apiKey)

	return &OpenAILLM{
		client: client,
		model:  model,
	}
}

// Summarise implements the LLM interface for OpenAI
func (o *OpenAILLM) Summarise(context string) (string, error) {
	prompt := fmt.Sprintf(`Please provide a concise summary of the following pull requests. Focus on the key changes, impact, and any notable patterns or themes:

%s

Please provide a summary in 2-3 paragraphs that would be useful for a development team's periodic report.`, context)

	resp, err := o.client.CreateChatCompletion(
		stdcontext.Background(),
		openai.ChatCompletionRequest{
			Model: o.model,
			Messages: []openai.ChatCompletionMessage{
				{
					Role:    openai.ChatMessageRoleUser,
					Content: prompt,
				},
			},
			MaxTokens:   500,
			Temperature: 0.7,
		},
	)
	if err != nil {
		return "", fmt.Errorf("OpenAI API error: %w", err)
	}

	if len(resp.Choices) == 0 {
		return "", fmt.Errorf("no response from OpenAI")
	}

	return strings.TrimSpace(resp.Choices[0].Message.Content), nil
}

// OllamaLLM implements the LLM interface using Ollama's local API
type OllamaLLM struct {
	baseURL string
	model   string
	client  *http.Client
}

// OllamaRequest represents the request structure for Ollama API
type OllamaRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
	Stream bool   `json:"stream"`
}

// OllamaResponse represents the response structure from Ollama API
type OllamaResponse struct {
	Response string `json:"response"`
	Done     bool   `json:"done"`
	Error    string `json:"error,omitempty"`
}

// NewOllamaLLM creates a new Ollama LLM client
func NewOllamaLLM(baseURL, model string) *OllamaLLM {
	if baseURL == "" {
		baseURL = "http://localhost:11434" // Default Ollama URL
	}
	if model == "" {
		model = "llama3.2" // Default model
	}

	return &OllamaLLM{
		baseURL: baseURL,
		model:   model,
		client:  &http.Client{},
	}
}

// Summarise implements the LLM interface for Ollama
func (o *OllamaLLM) Summarise(context string) (string, error) {
	prompt := fmt.Sprintf(`Please provide a concise summary of the following pull requests. Focus on the key changes, impact, and any notable patterns or themes:

%s

Please provide a summary in 2-3 paragraphs that would be useful for a development team's periodic report.`, context)

	reqBody := OllamaRequest{
		Model:  o.model,
		Prompt: prompt,
		Stream: false,
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %w", err)
	}

	url := fmt.Sprintf("%s/api/generate", o.baseURL)
	resp, err := o.client.Post(url, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return "", fmt.Errorf("ollama API error: %w", err)
	}
	defer func() {
		if closeErr := resp.Body.Close(); closeErr != nil {
			// Log the error but don't fail the function
			fmt.Fprintf(os.Stderr, "Warning: failed to close response body: %v\n", closeErr)
		}
	}()

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("ollama API returned status %d", resp.StatusCode)
	}

	var ollamaResp OllamaResponse
	if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
		return "", fmt.Errorf("failed to decode response: %w", err)
	}

	if ollamaResp.Error != "" {
		return "", fmt.Errorf("ollama error: %s", ollamaResp.Error)
	}

	return strings.TrimSpace(ollamaResp.Response), nil
}
